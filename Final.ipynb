{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "import joblib\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    '''\n",
    "    Input  -> Raw Pandas Dataframe\n",
    "    Output -> Cleaned Pandas Dataframe\n",
    "    Task   -> This function keeps only those rows in the dataframe with prices that the Mercari Platform allows\n",
    "    '''\n",
    "    df = df[(df['price'] >= 3) & (df['price'] <= 2000)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(sent):\n",
    "    '''\n",
    "    Input  -> Raw text (string)\n",
    "    Output -> Cleaned Text (string)\n",
    "    Task   -> The objective of this function is to clean the text and make it suitable for Bag of Words/TF-IDF vectorization\n",
    "              This includes removal of new lines, special characters, emojis etc.\n",
    "    \n",
    "    '''\n",
    "    # Decontraction\n",
    "    sent = re.sub(r\"aren\\'t\", \"are not\", sent)\n",
    "    sent = re.sub(r\"didn\\'t\", \"did not\", sent)\n",
    "    sent = re.sub(r\"can\\'t\", \"can not\", sent)\n",
    "    sent = re.sub(r\"couldn\\'t\", \"could not\", sent)\n",
    "    sent = re.sub(r\"won\\'t\", \"would not\", sent)\n",
    "    sent = re.sub(r\"wouldn\\'t\", \"would not\", sent)\n",
    "    sent = re.sub(r\"haven\\'t\", \"have not\", sent)\n",
    "    sent = re.sub(r\"shouldn\\'t\", \"should not\", sent)\n",
    "    sent = re.sub(r\"doesn\\'t\", \"does not\", sent)\n",
    "    sent = re.sub(r\"don\\'t\", \"do not\", sent)\n",
    "    sent = re.sub(r\"didn\\'t\", \"did not\", sent)\n",
    "    sent = re.sub(r\"mustn\\'t\", \"must not\", sent)\n",
    "    sent = re.sub(r\"needn\\'t\", \"need not\", sent)\n",
    "    \n",
    "    #Removing special characters\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "\n",
    "    #Removing all special characters except the period\n",
    "    sent = regex_special_chars.sub(' ', sent)\n",
    "    \n",
    "    #Removing periods which are not either followed or preceeded by a digit\n",
    "    #Ref: https://stackoverflow.com/questions/6599646/remove-decimal-point-when-not-between-two-digits\n",
    "    \n",
    "    sent = regex_decimal_digits.sub(' ', sent)\n",
    "    \n",
    "    #Converting multiple white spaces to single white space\n",
    "    sent = regex_white_space.sub(' ', sent)\n",
    "    \n",
    "    #Removing space at starting and ending and converting to lower case\n",
    "    sent = sent.strip().lower()\n",
    "    \n",
    "    sent_list = sent.split()\n",
    "    \n",
    "    lem = WordNetLemmatizer()\n",
    "    text = [lem.lemmatize(word) for word in sent_list if word not in stop_words] \n",
    "    sent = \" \".join(text)\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining some special regexes which would be used in the function text_preprocessing() to clean the text\n",
    "regex_special_chars = re.compile('[^A-Za-z0-9.]+')\n",
    "regex_decimal_digits = re.compile('(?<!\\d)\\.(?!\\d)')\n",
    "regex_white_space = re.compile(r'\\s+')           \n",
    "    \n",
    "#Creating a slightly modified list of stopwords which does not contain \"no\", \"nor\" or \"not\"\n",
    "stop_words = set(stopwords.words(\"english\")) - {\"no\", \"nor\", \"not\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    '''\n",
    "    This function does preprocessing of the DataFrame including tasks like imputing the null values, joining mulitple columns\n",
    "    together and then applying the text_preprocessing() method defined above\n",
    "    Input  -> Pandas DataFrame (Raw)\n",
    "    Output -> Pandas DataFrame (Cleaned)\n",
    "    Task   -> Dealing with null values, Cleaning 'item_description' column, joining together various text fields and performing\n",
    "              text preprocessing defined above\n",
    "    '''\n",
    "\n",
    "    #Fill Null values with ''\n",
    "    df.fillna('', inplace = True)\n",
    "    \n",
    "    #Convert No description yet to ' '\n",
    "    df['item_description']  = df['item_description'].str.replace('^no description yet$', '', regex=True)\n",
    "    \n",
    "    #Combine various text fields to one single field is inspired from Kaggle Winners' solution\n",
    "    #This helps in controlling the number of features generated when vectorization is applied on a text column\n",
    "    #Ref: https://github.com/pjankiewicz/mercari-solution\n",
    "    \n",
    "    df['name'] = df['name'] + \" \" + df['brand_name']\n",
    "    df['text'] = df['item_description'] + \" \" + df['name'] + \" \" + df['category_name']\n",
    "    \n",
    "    df[['name', 'text']] =  df[['name', 'text']].applymap(lambda x : text_preprocessing(x))\n",
    "    \n",
    "    return df[['name', 'text', 'shipping', 'item_condition_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_encoder(train_data, test_data, N_GRAMS = 1):\n",
    "    '''\n",
    "    This function returns the TF-IDF encoding of the text\n",
    "    \n",
    "    Input ->\n",
    "    \n",
    "        train_data       : Text (string or list of strings or Pandas Series with elements as strings)\n",
    "        test_data        : Text (string or list of strings or Pandas Series with elements as strings)\n",
    "        N_GRAMS(int)     : Upper bound of the n_grams to be considered while vectorizing the data using TF-IDF encoder\n",
    "                           For eg., If the n_grams = 2, then both unigrams and bi-grams will be used while vectorizing\n",
    "                           the text data. Default value is kept as 1, which means only uni-grams will be generated if this\n",
    "                           argument is not supplied explicitly while calling this function\n",
    "    \n",
    "    Output -> Tuple of TF-IDF vectors of \"train_data\" and \"test_data\" computed using sklearn's Tfidfvectorizer() \n",
    "    \n",
    "    Task   -> Given a text (string), return the TF-IDF vectors for that text\n",
    "              The vectorizer is fitted on the train_data and used to tranform both the train data and the test data\n",
    "    '''\n",
    "    vectorizer = TfidfVectorizer(min_df = 3,\n",
    "                                 max_features = 100000,\n",
    "                                 ngram_range = (1, N_GRAMS),\n",
    "                                 strip_accents = 'unicode',\n",
    "                                 analyzer = 'word',\n",
    "                                 token_pattern = r'\\w{1,}')\n",
    "    \n",
    "    train_tdidf = vectorizer.fit_transform(train_data)\n",
    "    test_tfidf =  vectorizer.transform(test_data)\n",
    "    return (train_tdidf, test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(train_data, test_data):\n",
    "    '''\n",
    "    This function returns the One Hot Encoded vectors for the given train data and test data\n",
    "    Input ->\n",
    "        train_data : Training data to be fitted on and one hot encoded (List of integers/strings or a Pandas Series)\n",
    "        test_data  : Testing data to be one hot encoded (List of integers/strings or a Pandas Series)\n",
    "    Output -> Tuple of one hot encoded vectors of \"train_data\" and \"test_data\"\n",
    "    Task   -> This function converts the raw values (integers/strings) into one hot encoded vectors using\n",
    "              sklearn's OneHotEncoder()\n",
    "    '''\n",
    "    ohe_encoder = OneHotEncoder()\n",
    "    train_ohe   = ohe_encoder.fit_transform(train_data)\n",
    "    test_ohe    = ohe_encoder.transform(test_data)\n",
    "    return (train_ohe, test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model_1(train_shape):\n",
    "    '''\n",
    "    Task  -> This function builds the architecture of an MLP model with the input dimensions as \"train_shape\"\n",
    "             The architecture of the model is as follows:\n",
    "             Input Layer -> Dense (256) -> Dense (128) -> Dense (1) -> Output Layer\n",
    "             The activation function is kept as ReLu for the hidden layers and linear activation (f(x) = x) for the output layer\n",
    "    \n",
    "    Input  -> train_shape: Input shape (dimensions) of the data which will be fed to the MLP\n",
    "    \n",
    "    Output -> Builded MLP Model\n",
    "    '''\n",
    "    model_input = Input(shape=(train_shape,), dtype='float32', sparse=True)\n",
    "    out = Dense(256, activation='relu')(model_input)\n",
    "    out = Dense(128, activation='relu')(out)\n",
    "    model_out = Dense(1)(out)\n",
    "    model = Model(model_input, model_out)\n",
    "    return model\n",
    "    \n",
    "def mlp_model_2(train_shape):\n",
    "    '''\n",
    "    Task  -> This function builds the architecture of an MLP model with the input dimensions as \"train_shape\"\n",
    "             The architecture of the model is as follows:\n",
    "             Input Layer -> Dense (1024) -> Dense (512) -> Dense (256) -> Dense (128) -> Dense (64) -> Dense (32) -> Dense (1)\n",
    "             -> Output Layer\n",
    "             The activation function is kept as ReLu for the hidden layers and linear activation (f(x) = x) for the output layer\n",
    "    \n",
    "    Input  -> train_shape: Input shape (dimensions) of the data which will be fed to the MLP\n",
    "    \n",
    "    Output -> Builded MLP Model\n",
    "    '''\n",
    "    model_input = Input(shape=(train_shape,), dtype='float32', sparse=True)\n",
    "    out = Dense(1024, activation='relu')(model_input)\n",
    "    out = Dense(512, activation='relu')(out)\n",
    "    out = Dense(256, activation='relu')(out)\n",
    "    out = Dense(128, activation='relu')(out)\n",
    "    out = Dense(64, activation='relu')(out)\n",
    "    out = Dense(32, activation='relu')(out)\n",
    "    out = Dense(1)(out)\n",
    "    model = Model(model_input, out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_function(test_datapoint):\n",
    "    '''\n",
    "    Input:  Single Datapoint\n",
    "    Output: Predicted price\n",
    "    Task:   This function takes a single datapoint as input and returns the target variable, i.e., the predicted price for that\n",
    "            datapoint\n",
    "    '''\n",
    "    # Load Training Data\n",
    "    df_train = pd.read_csv('train.tsv', sep='\\t')\n",
    "    \n",
    "    # Clean Training Data\n",
    "    df_train = clean_df(df_train)\n",
    "    \n",
    "    # Extract the price column that will be used while training models\n",
    "    y_train     = df_train['price'].values\n",
    "    \n",
    "    # Taking log of the price column so that we can directly optimize for RMSLE\n",
    "    y_train_log = (np.log1p(y_train)).reshape((-1, 1))\n",
    "    \n",
    "    # Prprocess Training Data\n",
    "    df_train = preprocess_df(df_train)\n",
    "    \n",
    "    # Preprocess Testing Data\n",
    "    test_datapoint = preprocess_df(test_datapoint)\n",
    "    \n",
    "    # TF-IDF Encoding \"name\" and \"text\" columns\n",
    "    train_tfidf_vectors_name, test_tfidf_vectors_name = tfidf_encoder(df_train['name'].values,\n",
    "                                                                      test_datapoint['name'].values,\n",
    "                                                                      N_GRAMS = 1)\n",
    "    \n",
    "    train_tfidf_vectors_text, test_tfidf_vectors_text = tfidf_encoder(df_train['text'].values,\n",
    "                                                                      test_datapoint['text'].values,\n",
    "                                                                      N_GRAMS = 2)\n",
    "    \n",
    "    # One Hot Encoding item_condition_id and shipping columns\n",
    "    train_item_condition, test_item_condition = one_hot_encoder(np.reshape(df_train['item_condition_id'].values, (-1, 1)),\n",
    "                                                                np.reshape(test_datapoint['item_condition_id'].values, (-1, 1)))\n",
    "    \n",
    "    train_shipping, test_shipping             = one_hot_encoder(np.reshape(df_train['shipping'].values, (-1, 1)),\n",
    "                                                                np.reshape(test_datapoint['shipping'].values, (-1, 1)))\n",
    "    \n",
    "    # Combining all the encoded features to create the final train and test data matrices\n",
    "    X_train = hstack((train_tfidf_vectors_name,\n",
    "                      train_tfidf_vectors_text,\n",
    "                      train_item_condition,\n",
    "                      train_shipping)).tocsr().astype('float32')\n",
    "\n",
    "    X_test  = hstack((test_tfidf_vectors_name,\n",
    "                      test_tfidf_vectors_text,\n",
    "                      test_item_condition,\n",
    "                      test_shipping)).tocsr().astype('float32')\n",
    "\n",
    "    # Training MLP Models    \n",
    "    mlp1 = mlp_model_1(X_train.shape[1])\n",
    "    mlp1.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    mlp1.fit(X_train, y_train_log, batch_size = 256, epochs = 1, verbose = 1)\n",
    "    mlp1.fit(X_train, y_train_log, batch_size = 512, epochs = 1, verbose = 1)\n",
    "    mlp1.fit(X_train, y_train_log, batch_size = 1024, epochs = 1, verbose = 1)\n",
    "\n",
    "    mlp2 = mlp_model_2(X_train.shape[1])\n",
    "    mlp2.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    mlp2.fit(X_train, y_train_log, batch_size = 256, epochs = 1, verbose = 1)\n",
    "    mlp2.fit(X_train, y_train_log, batch_size = 512, epochs = 1, verbose = 1)\n",
    "    mlp2.fit(X_train, y_train_log, batch_size = 1024, epochs = 1, verbose = 1)\n",
    "    \n",
    "    # Obtaining predictions from MLP-1 and MLP-2\n",
    "    y_pred_mlp_1 = np.expm1((mlp1.predict(X_test)[:, 0]).reshape(-1, 1))\n",
    "    y_pred_mlp_2 = np.expm1((mlp2.predict(X_test)[:, 0]).reshape(-1, 1))\n",
    "    \n",
    "    pred_final = 0.42*mlp1_preds + 0.58*mlp2_preds\n",
    "    \n",
    "    return pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1():\n",
    "    '''\n",
    "    This function is used to calculate the predicted price for an input datapoint (selected from the test set)\n",
    "    '''\n",
    "    df_test        = pd.read_csv('test.tsv', sep='\\t')\n",
    "    test_datapoint = df_test[21:22]\n",
    "    pred_final     = pipeline_function(test_datapoint)\n",
    "    print(\"Input datapoint is: \\n\", test_datapoint.values, \"\\n\")\n",
    "    print(\"Final predicted price for the given datapoint is =\", np.round(pred_final, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5788/5788 [==============================] - 33s 6ms/step - loss: 0.2274\n",
      "2894/2894 [==============================] - 21s 7ms/step - loss: 0.1502\n",
      "1447/1447 [==============================] - 13s 9ms/step - loss: 0.1092\n",
      "5788/5788 [==============================] - 85s 15ms/step - loss: 0.2187\n",
      "2894/2894 [==============================] - 47s 16ms/step - loss: 0.1318\n",
      "1447/1447 [==============================] - 29s 20ms/step - loss: 0.0712\n",
      "Input datapoint is: \n",
      " [[21 'iPhone 6:6S Case Marble **SALE*' 1\n",
      "  'Electronics/Cell Phones & Accessories/Cases, Covers & Skins' 'Apple' 1\n",
      "  '**MESSAGE THE MARBLE COLOR AND SIZE YOU WOULD LIKE AFTER YOU ORDER** BUNDLE PROMOTION: ONE FOR [rm] OR PICK ANY TWO COLORS FOR ONLY [rm]!! (Get one for yourself and another for your friend or family!) Up for sale is a BRAND NEW Marble Pattern Case (Color Options: White/Black/Milky Blue/Pink) Designed Perfectly to fit BOTH iPhone 6 & 6S 4.7 ALSO AVAILABLE for iPhone 6/6S PLUS MODELS\" Design Spec: - Easy Access to ALL ports - High Quality TPU Shell Shock Proof - Slim and Light Weighto - 360 Silicone Grip & Protection FAST FREE SHIPPING! All our products are shipped via USPS First Class with real- time TRACKING! CUSTOMER SERVICE We are an experienced seller and for us customer satisfaction is our priority. We work HARD to make sure all our customers are 100% satisfied! We respond quick so feel free to reach out to us with any questions! PRICED TO SELL ALREADY DISCOUNTED 15%!! ORDER SOON LIMITED QUANTITY!!']] \n",
      "\n",
      "Final predicted price for the given datapoint is = [[9.07]]\n"
     ]
    }
   ],
   "source": [
    "function_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    '''\n",
    "    This function take the tuple of true class labels and the predicted class label as input and gives the Root mean squared\n",
    "    log error between these as the output\n",
    "    '''\n",
    "    return np.sqrt((np.log1p(y_true) - np.log1p(y_pred))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2():\n",
    "    '''\n",
    "    This function is used to calculate the predicted price for an input datapoint (selected from the train set) and the\n",
    "    associated RMSLE calculated with the original datapoint\n",
    "    '''\n",
    "    df_train = pd.read_csv('train.tsv', sep='\\t')\n",
    "    df_train = clean_df(df_train)\n",
    "    test_datapoint = df_train[1:2]\n",
    "    price_original = test_datapoint['price'].values[0]\n",
    "    pred_final = pipeline_function(test_datapoint)[0][0]\n",
    "    print(\"Input datapoint is: \\n\", test_datapoint.values[0], \"\\n\")\n",
    "    print(\"Final predicted price for the given datapoint is =\", np.round(pred_final, 2))\n",
    "    print(\"RMSLE for the given datapoint is = \", rmsle(pred_final, price_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5788/5788 [==============================] - 35s 6ms/step - loss: 0.2267\n",
      "2894/2894 [==============================] - 20s 7ms/step - loss: 0.1485\n",
      "1447/1447 [==============================] - 14s 10ms/step - loss: 0.1073\n",
      "5788/5788 [==============================] - 86s 15ms/step - loss: 0.2172\n",
      "2894/2894 [==============================] - 48s 17ms/step - loss: 0.1314\n",
      "1447/1447 [==============================] - 30s 21ms/step - loss: 0.0709\n",
      "Input datapoint is: \n",
      " [1 'Razer BlackWidow Chroma Keyboard' 3\n",
      " 'Electronics/Computers & Tablets/Components & Parts' 'Razer' 52.0 0\n",
      " 'This keyboard is in great condition and works like it came out of the box. All of the ports are tested and work perfectly. The lights are customizable via the Razer Synapse app on your PC.'] \n",
      "\n",
      "Final predicted price for the given datapoint is = 56.21\n",
      "RMSLE for the given datapoint is =  0.07643717427746788\n"
     ]
    }
   ],
   "source": [
    "function_2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
